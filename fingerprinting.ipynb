{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Fingerprinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a Python class named AudioFingerprinting for creating an audio fingerprinting system, leveraging the LibriSpeech dataset inbuilt in torchvision for testing. \n",
    "\n",
    "The code is designed to work with audio files, extracting features using Mel-frequency cepstral coefficients (MFCC) and identifying similar audio clips within the database based on these features. Here's a brief overview of its functionalities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building features database: 100%|██████████| 2703/2703 [00:18<00:00, 144.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features\n",
      "Finding Matches\n",
      "\n",
      "Input Speaker ID: 1462, Path: 1462-170138-0027, Chapter ID: 170138, Utterance ID: 27\n",
      "\n",
      "Top Matches:\n",
      "Match |  Speaker   | Chapter  | Utterance  | Score  |  Audio Filename \n",
      "-------------------------------------------------------------------------------\n",
      "1     |    1462    |  170138  |     27     | 1.0000 | 1462-170138-0027\n",
      "2     |    1462    |  170145  |     16     | 0.9997 | 1462-170145-0016\n",
      "3     |    1462    |  170142  |     15     | 0.9997 | 1462-170142-0015\n",
      "4     |    1462    |  170138  |     6      | 0.9995 | 1462-170138-0006\n",
      "5     |    1462    |  170138  |     24     | 0.9995 | 1462-170138-0024\n",
      "6     |    1462    |  170142  |     11     | 0.9994 | 1462-170142-0011\n",
      "7     |    1462    |  170145  |     22     | 0.9994 | 1462-170145-0022\n",
      "8     |    1462    |  170142  |     16     | 0.9993 | 1462-170142-0016\n",
      "9     |    1462    |  170138  |     16     | 0.9993 | 1462-170138-0016\n",
      "10    |    1462    |  170138  |     3      | 0.9993 | 1462-170138-0003\n"
     ]
    }
   ],
   "source": [
    "class AudioFingerprinting:\n",
    "    \"\"\"\n",
    "    A class for creating an audio fingerprinting system that builds a database of audio features\n",
    "    from the LibriSpeech dataset and finds top matches for a given audio input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initializes the audio fingerprinting system.\n",
    "\n",
    "        Parameters:\n",
    "        - root_dir: str, the directory where the LibriSpeech dataset will be stored.\n",
    "        - device: str, the device to use for computations. Defaults to 'cuda' if available.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(root=root_dir, url=\"dev-clean\", download=True)\n",
    "\n",
    "        # Initialize an empty list to store the database of audio features and metadata.\n",
    "        self.database = []\n",
    "        self.mfcc_transform = transforms.MFCC(sample_rate=16000, n_mfcc=12).to(self.device)\n",
    "\n",
    "    def extract_features(self, waveform):\n",
    "        \"\"\"\n",
    "        Extracts MFCC features from a waveform.\n",
    "\n",
    "        Parameters:\n",
    "        - waveform: Tensor, the audio waveform.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor, the extracted MFCC features.\n",
    "        \"\"\"\n",
    "        waveform = waveform.to(self.device)\n",
    "        mfcc = self.mfcc_transform(waveform).mean(dim=2)\n",
    "        return mfcc.squeeze()\n",
    "\n",
    "    def build_features_database(self):\n",
    "        \"\"\"\n",
    "        Builds a database of features and metadata for each audio file in the dataset.\n",
    "        \"\"\"\n",
    "        for i, data in enumerate(tqdm(self.dataset, desc=\"Building features database\")):\n",
    "            waveform, _, _, speaker_id, chapter_id, utterance_id = data\n",
    "            # Retrieve the file path for reference.\n",
    "            file_path = os.path.basename(self.dataset._walker[i])\n",
    "            features = self.extract_features(waveform)\n",
    "            # Append the features and metadata to the database.\n",
    "            self.database.append({\n",
    "                'features': features.unsqueeze(0),\n",
    "                'speaker_id': speaker_id,\n",
    "                'chapter_id': chapter_id,\n",
    "                'utterance_id': utterance_id,\n",
    "                'path': file_path\n",
    "            })\n",
    "\n",
    "    def find_top_matches(self, input_features, top_n=5):\n",
    "        \"\"\"\n",
    "        Finds the top N matches for a given set of input features in the database.\n",
    "\n",
    "        Parameters:\n",
    "        - input_features: Tensor, the features of the input audio to match.\n",
    "        - top_n: int, the number of top matches to return.\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, each containing the database entry and similarity score for a match.\n",
    "        \"\"\"\n",
    "        input_features = input_features.to(self.device).unsqueeze(0)\n",
    "        # Calculate the cosine similarity between input features and each entry in the database.\n",
    "        similarities = [torch.cosine_similarity(input_features, entry['features'], dim=1).item() for entry in self.database]\n",
    "        # Get indices of the top N matches.\n",
    "        top_n_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_n]\n",
    "        # Retrieve the top matches and their scores.\n",
    "        top_matches = [(self.database[idx], similarities[idx]) for idx in top_n_indices]\n",
    "\n",
    "        return top_matches\n",
    "\n",
    "    @staticmethod\n",
    "    def print_table(matches):\n",
    "        \"\"\"\n",
    "        Prints a formatted table of the top matches. \n",
    "\n",
    "        Parameters:\n",
    "        - matches: List of tuples, the top matches to print.\n",
    "        \"\"\"\n",
    "        max_path_len = max(len(match['path']) for match, _ in matches)\n",
    "        path_col_width = max(max_path_len, 10)  # Ensure a minimum column width for the audio filename.\n",
    "\n",
    "        # Define the header and row format with appropriate spacing.\n",
    "        header_format = f\"{{:<5}} | {{:^10}} | {{:^8}} | {{:^10}} | {{:^6}} | {{:^{path_col_width}}}\"\n",
    "        row_format = f\"{{:<5}} | {{:^10}} | {{:^8}} | {{:^10}} | {{:^6.4f}} | {{:{path_col_width}}}\"\n",
    "\n",
    "        print(header_format.format('Match', 'Speaker', 'Chapter', 'Utterance', 'Score', 'Audio Filename'))\n",
    "        print(\"-\" * (48 + path_col_width + (5 * 3)))  # Adjust the total length to account for separators.\n",
    "        \n",
    "        for i, (match, score) in enumerate(matches, start=1):\n",
    "            print(row_format.format(i, match['speaker_id'], match['chapter_id'], match['utterance_id'], score, match['path']))\n",
    "\n",
    "# Example usage\n",
    "root_dir = './data'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "afp = AudioFingerprinting(root_dir)\n",
    "print('Building database')\n",
    "afp.build_features_database()\n",
    "\n",
    "# Take a sample audio and extract it features\n",
    "sample_id = 100\n",
    "input_waveform, _, _, input_label, input_chapter_id, input_utterance_id = afp.dataset[sample_id]\n",
    "input_file_path = afp.dataset._walker[sample_id]\n",
    "print('Extracting Features')\n",
    "input_features = afp.extract_features(input_waveform)\n",
    "\n",
    "# Find top n similar audio \n",
    "print('Finding Matches')\n",
    "top_matches = afp.find_top_matches(input_features, top_n=10)\n",
    "print(f\"\\nInput Speaker ID: {input_label}, Path: {input_file_path}, Chapter ID: {input_chapter_id}, Utterance ID: {input_utterance_id}\")\n",
    "print(\"\\nTop Matches:\")\n",
    "afp.print_table(top_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Observation\n",
    "- All top matches belong to the same speaker (Speaker ID: 1462), demonstrating that the system is very effective in recognizing and matching features specific to a speaker's voice. This suggests that the MFCC features extracted are robust indicators of speaker characteristics.\n",
    "\n",
    "- The fact that the input audio matches itself with a perfect score as the first result is a sanity check for the system, confirming that when the exact audio is present in the database, the system will indeed recognize and rank it as the most similar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
